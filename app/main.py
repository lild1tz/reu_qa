import os
import subprocess
import threading
from fastapi import FastAPI
from fastapi.responses import RedirectResponse
from langchain_community.utilities import GoogleSerperAPIWrapper
from langchain_openai import ChatOpenAI
from app.models import QueryRequest, ResponseModel
from app.config import Config
from app.prompts import (
    prompt_template_relevant_to_reu, output_parser_relevant_to_reu,
    prompt_template_final, output_parser_final
    )
from app.utils import (
    relevant_to_reu, 
    general_output,
    search_info
)

search = GoogleSerperAPIWrapper(serper_api_key=Config.SERPER_API_KEY)
llm = ChatOpenAI(
    temperature=0.0,
    model=Config.OPENAI_MODEL_NAME,
    openai_api_key=Config.OPENAI_API_KEY,
    base_url=Config.OPENAI_BASE_URL
)

app = FastAPI()

# üîπ –ó–∞–ø—É—Å–∫–∞–µ–º Streamlit –≤ —Ñ–æ–Ω–æ–≤–æ–º –ø–æ—Ç–æ–∫–µ
def run_streamlit():
    os.system("streamlit run frontend/frontend.py --server.port 8501 --server.headless true")

threading.Thread(target=run_streamlit, daemon=True).start()

@app.get("/")
async def redirect_to_frontend():
    """ –ü–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å—Ä–∞–∑—É –≤ Streamlit-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å """
    return RedirectResponse(url="http://localhost:8501")

@app.post("/api/request")
async def handle_request(request: QueryRequest) -> ResponseModel:
    question = request.query
    relevant, reasoning = await relevant_to_reu(
        llm=llm, 
        question=question,
        promt_template=prompt_template_relevant_to_reu,
        output_parser=output_parser_relevant_to_reu
    )

    if not relevant:
        return ResponseModel(
            answer="–î–∞–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å –Ω–µ —Å–≤—è–∑–∞–Ω —Å –†–≠–£",
            reasoning=reasoning,
            sources=[]
        )

    search_results = await search_info(search=search, question=question, top_sites=5)
    context = '\n'.join(search_results[0]) if search_results[0] else "–ö–æ–Ω—Ç–µ–∫—Å—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç."
    links = search_results[1] if search_results[1] else []

    answer, reasoning = await general_output(
        llm=llm,
        question=question,
        context=context,
        promt_template=prompt_template_final,
        output_parser=output_parser_final
    )

    return ResponseModel(
        answer=answer,
        reasoning=reasoning,
        sources=links
    )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)